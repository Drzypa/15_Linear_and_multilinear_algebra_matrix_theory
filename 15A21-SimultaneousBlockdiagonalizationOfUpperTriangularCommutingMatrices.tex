\documentclass[12pt]{article}
\usepackage{pmmeta}
\pmcanonicalname{SimultaneousBlockdiagonalizationOfUpperTriangularCommutingMatrices}
\pmcreated{2013-03-22 15:29:42}
\pmmodified{2013-03-22 15:29:42}
\pmowner{lars_h}{9802}
\pmmodifier{lars_h}{9802}
\pmtitle{simultaneous block-diagonalization of upper triangular commuting matrices}
\pmrecord{5}{37353}
\pmprivacy{1}
\pmauthor{lars_h}{9802}
\pmtype{Theorem}
\pmcomment{trigger rebuild}
\pmclassification{msc}{15A21}
%\pmkeywords{block-diagonalization}
%\pmkeywords{block-diagonalisation}

\endmetadata

\usepackage{amsmath,amsfonts,amssymb,amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}


\newcommand{\mc}{\mathcal}
\newcommand{\vek}{\mathbf}

\newcommand{\Mat}{\mathrm{M}}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\Trans}[1]{#1^{\mathrm{T}}\!}
\begin{document}
\PMlinkescapeword{column}
\PMlinkescapeword{row}
\PMlinkescapeword{independent}
\PMlinkescapeword{order}
\PMlinkescapeword{complete}
\PMlinkescapeword{satisfy}

Let $\vek{e}_i$ denote the (column) vector whose $i$th position is $1$ 
and where all other positions are $0$. Denote by $[n]$ the set 
$\{1,\dotsc,n\}$. Denote by $\Mat_n(\mc{K})$ the set of all $n \times 
n$ matrices over $\mc{K}$, and by $\GL_n(\mc{K})$ the set of all 
invertible elements of $\Mat_n(\mc{K})$. Let $d_i$ be the function 
which extracts the $i$th diagonal element of a matrix, i.e., \(d_i(A) 
= \Trans{\vek{e}_i} A \vek{e}_i\).

\begin{theorem}
  Let $\mc{K}$ be a field, let $n$ be a positive integer, and let $\sim$ 
  be an equivalence relation on $[n]$ such that if \(i \sim j\) and 
  \(i \leqslant k \leqslant j\) then \(k \sim i\). Let \(A_1,\dotsc,A_r 
  \in \Mat_n(\mc{K})\) be pairwise commuting upper triangular matrices. 
  If these matrices and $\sim$ are related such that
  \begin{equation*}
    i \sim j
    \quad\text{if and only if}\quad
    d_i(A_k) = d_j(A_k)
    \text{ for all }k \in [r]\text{,}
  \end{equation*}
  then there exists a matrix \(B \in \GL_n(\mc{K})\) such that:
  \begin{enumerate}
    \item \label{Cond:Block}
      If \(\Trans{\vek{e}_i} B^{-1} A_k B \vek{e}_j \neq 0\) then 
      \(i \sim j\) and \(i \leqslant j\).
    \item \label{Cond:Same}
      If \(i \sim j\) then \(\Trans{\vek{e}_i} B^{-1} A_k B 
      \vek{e}_j = \vek{e}_i^{\mathrm{T}} A_k \vek{e}_j\).
  \end{enumerate}
\end{theorem}

Condition~\ref{Cond:Block} says that if an element of $B^{-1}A_kB$ 
is nonzero then both its row and column indices must belong to the 
same equivalence class of $\sim$, i.e., the nonzero elements of 
$B^{-1}A_kB$ only occur in particular 
\PMlinkname{blocks}{PartitionedMatrix} \PMlinkescapeword{blocks}
along the diagonal, and these blocks 
correspond to equivalence classes of $\sim$. 
Condition~\ref{Cond:Same} says that within one of these blocks, 
$B^{-1}A_kB$ is equal to $A_k$.

The proof of the theorem requires the following lemma.

\begin{lemma} \label{L:Kar.Matris}
  Let a sequence \(A_1,\dotsc,A_r \in \Mat_n(\mc{K})\) of upper 
  triangular matrices be given, and denote by $\mc{A}$ the 
  \PMlinkname{unital}{unity} \PMlinkname{algebra}{Algebra} 
  generated by these matrices. For every sequence 
  \(\lambda_1,\dotsc,\lambda_r \in \mc{K}\) of scalars there exists a 
  matrix \(C \in \mc{A}\) such that
  $$
    d_i(C) = \begin{cases}
      1& \text{if \(d_i(A_k)=\lambda_k\) for all \(k \in [r]\),}\\
      0& \text{otherwise}
    \end{cases}
  $$
  for all \(i \in [n]\).
\end{lemma}

The proof of that lemma can be found in 
\PMlinkname{this article}{CharacteristicMatrixOfDiagonalElementCrossSection}.


\begin{proof}[Proof of theorem]
  The proof is by induction on the number of equivalence classes of 
  $\sim$. If there is only one equivalence class then one can take 
  \(B=I\).
  
  If there is more than one equivalence class, then let $S$ be the 
  equivalence class that contains $n$. By Lemma~\ref{L:Kar.Matris} 
  there exists a matrix $C$ in the unital algebra generated by 
  $A_1,\dotsc,A_r$ (hence necessarily upper triangular) such that 
  \(d_i(C) = 1\) for all \(i \in S\) and \(d_i(C)=0\) for all \(i \in 
  [n] \setminus S\). Thus $C$ has a 
  \PMlinkescapetext{block decomposition}
  \[
    C = \begin{pmatrix} C_{11}& C_{12} \\ 0& C_{22} \end{pmatrix}
  \]
  where $C_{22}$ is a $\lvert S\rvert \times \lvert S\rvert$ matrix 
  that has all $1$s on the diagonal, and $C_{11}$ is a 
  $\bigl( n -\nobreak \lvert S\rvert \bigr) \times
  \bigl( n -\nobreak \lvert S\rvert \bigr)$ matrix that has all $0$s 
  on the diagonal.
  
  Let \(k \in [r]\) be arbitrary and similarly decompose
  \begin{align*}
    C^n ={}& \begin{pmatrix} D_{11}& D_{12} \\ 0& D_{22} \end{pmatrix} 
    \text{,}&
    A_k ={}& \begin{pmatrix} A_{11}& A_{12} \\ 0& A_{22} \end{pmatrix} 
    \text{.}
  \end{align*}
  One can identify \(D_{11} = (C_{11})^n\) and \(D_{22} = (C_{22})^n\), 
  but due to the zero diagonal of $C_{11}$ and the fact that the 
  \PMlinkescapetext{sides} 
  of these matrices are smaller than $n$, the more striking equality 
  \(D_{11}=0\) also holds. As for $D_{22}$, one may conclude that it 
  is invertible.
  
  Since the algebra that $C^n$ belongs to was generated by pairwise 
  commuting elements, it is a \PMlinkname{commutative}{Commutative} 
  algebra, and in particular \(C^n A_k = A_k C^n\). In 
  \PMlinkescapetext{terms} 
  of the individual blocks, this 
  \PMlinkescapetext{identity} 
  becomes
  \begin{equation*}
    \begin{pmatrix}
      0& D_{12}A_{22} \\
      0& D_{22}A_{22}
    \end{pmatrix} = \begin{pmatrix}
      0& A_{11}D_{12} + A_{12}D_{22} \\
      0& A_{22}D_{22}
    \end{pmatrix}
    \text{.}
  \end{equation*}
  Now let
  \begin{equation*}
    D = \begin{pmatrix} I& D_{12} \\ 0& D_{22} \end{pmatrix}
    \text{, so that }
    D^{-1} = \begin{pmatrix} 
      I& -D_{12}D_{22}^{-1} \\ 
      0& D_{22}^{-1}
    \end{pmatrix}
  \end{equation*}
  and consider the matrix \(D^{-1}A_kD\). Clearly
  \begin{multline*}
    D^{-1}A_kD =
    D^{-1} \begin{pmatrix} A_{11}& A_{12} \\ 0& A_{22} \end{pmatrix}
      \begin{pmatrix} I& D_{12} \\ 0& D_{22} \end{pmatrix} = \\ =
    D^{-1} \begin{pmatrix}
      A_{11}& A_{11}D_{12} + A_{12}D_{22} \\
      0& A_{22} D_{22}
    \end{pmatrix} = 
    \begin{pmatrix} 
      I& -D_{12}D_{22}^{-1} \\ 0& D_{22}^{-1} 
    \end{pmatrix}
    \begin{pmatrix} 
      A_{11}& D_{12}A_{22} \\ 0& D_{22} A_{22}
    \end{pmatrix} = \\ =
    \begin{pmatrix} 
      A_{11}& D_{12}A_{22} -D_{12}D_{22}^{-1}D_{22}A_{22} \\
      0& D_{22}^{-1} D_{22} A_{22}
    \end{pmatrix} =
    \begin{pmatrix} 
      A_{11}& 0 \\
      0& A_{22}
    \end{pmatrix}
  \end{multline*}
  so that the positions with row not in $S$ and column in $S$ are all 
  zero, as requested for $B^{-1}A_kB$. It should be observed that the 
  choice of $D$ is independent of $k$, and that the same $D$ thus 
  works for all the $A_k$.
  
  In order to complete the proof, one applies the induction hypothesis 
  to the restriction of $\sim$ to $[n] \setminus S$ and the 
  corresponding submatrices of $D^{-1} A_k D$, which satisfy the same 
  conditions but have one equivalence class less. This produces a 
  block-diagonalising matrix $B'$ for these submatrices, and thus the 
  sought $B$ can be constructed as \(D \left( \begin{smallmatrix} 
  B'&0 \\ 0&I \end{smallmatrix} \right)\).
\end{proof}
%%%%%
%%%%%
\end{document}
